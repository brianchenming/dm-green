## Device Mapper
Device mapper, as a general purpose mapping subsystem, is essentially a 
middleware sitting between Generic Block Layer and block devices. 
It receives BIOs, which is a kernel structure (struct bio)
describes IO requests, from Generic Block Layer, then redirects them to other
block devices by updating the targeted device (bi_bdev) and target 
sector (bi_sector). Device mapper itself does not
perform the redirection. It assigns this operation to device mapper targets,
which are registered kernel modules performing certain kinds of mapping. This
assignment is specified and maintained in a mapping table. 
Example device mapper targets include and not limited to linear, crypt, multipath, 
stripe, raid, snap, replicator, etc. 

Device mapper falls appart into the following kernel and user space components:
+kernel
	- core
	- ioctl interface
	- io
	- kcopyd
	- mapping targets
	- dirty log
	- region hash

+user space
	- library (libdevmapper)
	- dmsetup tool


In Linux kernel, a mapping target is an instance of 'struct target_type', which
contains functions to construct/destruct mapped devices, maps IO requests, merges
adjacent IO requests, reports/controls status, as well as hooks of suspend/resume
and IO hints. 

In the case of green target: 
	'green_ctr' is the constructor
	'green_dtr' is the destructor 
	'green_map' is called by the framework to perform IO redirection
		- The first argument is a pointer of 'struct dm_target', which contains
		  a pointer to 'struct green_c'. 
		- 'struct green_c' is the context of our green target and it contains
		  all information including metadata, mapping table, data
	          for disk management, and clients of device-mapper infrastrture services. 

## Power management
IT power consumption is critical. Operating System, as the system level software, plays an 
important role in the fact that it is always running as long as the machine is on. 
Therefore, make the OS components green could save us power in the long run. 

Besides, different block devices (SATA, SAS, SSD, PCM, Shingled disk, etc) have different 
tradeoffs between energy and performance.  Could we provide a transparent block storage to 
the upper level applications that is aware of the power and performance issue? Or in 
another word, could we provide a device mapper that could dynamically redirect IO to 
different block devices to save energy while maintaining performance, even under the 
constraints of strong security? What the green device mapper may also do is to spin 
down/up disks/RAID by prefetching to save power.  In fact, the ops/j ratio for different 
devices are different, and the green device mapper may assign priority values to different 
devices to minimize the cost function composed of energy and performance.

## Disk Management
Physical disks are configured using parameters passed through dmsetup. They are
sorted by their energy efficency, i.e., the most energy-efficient disk goes
first and so on. A 'struct mapped_disk' represents a mapped disk. In fact, 
mapped disks are not necessarily physical disks, as they can be virtual disk 
provided by device-mapper by themselves. Because we are caring about energy
consumption of physical disks, we will just take mapped disks as physical. As the
first disk is the most energy-efficient one, it is taken specially and called
cache disk. 

Instead of managing space on disks in unit of sectors, green target divides
disks into larger unit named *extent*. This has the following effects either
good or bad:

NOTE: extent here works just like chunk, and the later is used more often in 
various situations. 

1. Smaller mapping table.

If mapping is done in units of sector, the table will be as large as 8GB with
4-bytes entries. It is apparently prohibitive for an in-memory (otherwise, too
    slow if the mapping itself incurs extra IO). Extent is also used in LVM,
  where a typical size of extent is 4MB. Then the mapping becomes extent-wise
  and its size diminishes to 1MB in the above example. 

2. More aggresive prefetch.

As energy-efficient disk such as SSD have similar effect as disk cache. When
a large extent of data is move onto SSD, it can be consider as an aggresive
prefetch. 

3. Coarse-grain I/O during data migration among disks, more sequential I/O. 

Because the major lantency of magnetic disk is seek time, a larger sequential
access will not significantly slow down the IO. Moreover, with large size of
migration unit, there are fewer IO because adjacent sectors can be grouped. This
is benefical to the life time of SSD as well considering its limited 
write-erase cycles.

4. Overhead can be potentially high. 

Since each extent can represent several sectors, depends on the workload IO, 
more sectors IO can be wasteful and only adds overhead to the overall system. 
Besides, it also has the defragmentation problem, and can be highly costy if 
explored badly.  

## Physical Extent Management
Physical extents are addressed linear. Considering two disks of size 8 in extent,
extent 0 is the first extent on disk 1 and extent 8 will be the first one on disk 
2. 

Usage of physical extents is recorded by 'bitmap' within 'green_c'. As the name
implies, it is a bitmap with 1 means used and 0 means free. The utilities for
allocating/freeing an extent are 'get_extent' and 'put_extent'.

Specifically, each physical extent on prime disk has a 'struct extent'. It is an
array pointed by the 'prime_extents' field within 'green_c'. Free extents are
linked in a list named 'prime_free' and used extents are linked in 'prime_use'
list. They are used for data migration. The utilities for allocating/freeing
an prime extent are 'get_prime' and 'put_prime'.

## Virtual Extent Management
Virtual extents are represented by 'struct vextent'. They reside in an array
pointed by 'table' in 'green_c'. Each 'struct vextent' contains the physical
extent address it is mapped ('eid'), number of access ('counter'), a timestamp
('tick') and states ('state').

## Extent Migration
There are two kinds of data migration. The first is moving an extent into the
prime disk, called promotion. The second is moving an extent out of the prime
disk, called demotion. Both of them use the kcopyd API for data movement between
disks.

## Promotion
Promotion occurs when an extent outside of prime disk becomes hot and there is
free extents on prime disk. It is initialized by 'promote_extent' and finished
by 'promote_callback'. 

## Demotion
Promotion occurs when the number of free prime extents falls below a threshold.
Currently, the threshold is 'EXTENT_LOW'. Demotion tries to evict cold extents
on prime disks until the number of free prime extents goes up to another
threshold, which currently is 'EXTENT_FREE'. It is initialized by
'demote_extent' and finished 'demote_extent'. For demotion, there is a demotion
daemon implemented as a workqueue 'kgreend_wq'. 
